{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio Interface:\n"
      ],
      "metadata": {
        "id": "xCT3BxEyXjVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "#!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
        "\n",
        "!pip install -q bitsandbytes\n",
        "!pip install -q evaluate jsonlines rouge_score bert-score\n",
        "!pip install transformers peft accelerate bitsandbytes jsonlines\n",
        "!pip install evaluate gradio\n",
        "import evaluate\n",
        "2"
      ],
      "metadata": {
        "id": "TLKhXjurR0CH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Choosing the model:"
      ],
      "metadata": {
        "id": "DFG83PY0XrSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "max_seq_length = 2048  # Choose any! We auto support ROPE scaling internally!\n",
        "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, bFloat16 for Ampere+\n",
        "\n",
        "Developed_model = [\n",
        "    \"davnas/Italian_Cousine\",                  # unsloth/Llama-3.2-1B-Instruct with 3 epochs\n",
        "    \"davnas/Italian_Cousine_1.2\",              # unsloth/Llama-3.2-1B-Instruct with 5 epochs\n",
        "    \"davnas/Italian_Cousine_1.3\",              # unsloth/Llama-3.2-1B-Instruct with 7 epochs\n",
        "    \"davnas/Italian_Cousine_2\",                # Llama-3.2-1B-Instruct-bnb-4bit with 3 epochs\n",
        "    \"davnas/Italian_Cousine_2.0\",              # Llama-3.2-1B-Instruct-bnb-4bit with 5 epochs\n",
        "    \"davnas/Italian_Cousine_2.1\",              # Llama-3.2-1B-Instruct-bnb-4bit with 7 epochs\n",
        "]\n",
        "\n",
        "model_name_or_path = \"davnas/Italian_Cousine\"\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name_or_path,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=True,\n",
        "    # token = \"hf_...\", #se il nostro modello non Ã¨ public\n",
        "    # Use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jO4kp7KRz-3",
        "outputId": "81868efb-f817-4a4b-cb4a-506d13a2d7a7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2024.12.4: Fast Llama patching. Transformers:4.46.3.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Interface:"
      ],
      "metadata": {
        "id": "vuplvvcgXybN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "26S-TsyWRzGs",
        "outputId": "ad351fa7-cbc5-4e21-c9e2-88d1c6bdbefa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/interface.py:399: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://13f2d29407afd5b4c6.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://13f2d29407afd5b4c6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import gradio as gr\n",
        "import re\n",
        "\n",
        "# Enable faster inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "def chatbot(user_input):\n",
        "    # Add a system role for better context\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a professional chef assistant who provides accurate and detailed recipes.\"},\n",
        "        {\"role\": \"user\", \"content\": user_input}\n",
        "    ]\n",
        "    # Tokenize inputs\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Generate output with an increased token limit\n",
        "    outputs = model.generate(inputs, max_new_tokens=256, temperature=0.7)  # Increased max_new_tokens\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the recipe content (skip system and user roles)\n",
        "    recipe_start = response.lower().find(\"preparation time\")  # Look for where the recipe content begins\n",
        "    if recipe_start != -1:\n",
        "        response = response[recipe_start:]  # Trim everything before the recipe content\n",
        "\n",
        "    # Bold specific labels like preparation time, cooking time, portions, etc.\n",
        "    response = re.sub(\n",
        "        r\"(preparation time:|cooking time:|portions:|ingredients:|procedure:|nutrients:)\",\n",
        "        r'<span style=\"font-size:1.2em; font-weight:bold;\">\\1</span>',\n",
        "        response,\n",
        "        flags=re.IGNORECASE\n",
        "    )\n",
        "\n",
        "    # Replace **text** with bold HTML tags and make the font size larger\n",
        "    formatted_response = re.sub(\n",
        "        r\"\\*\\*(.*?)\\*\\*\",\n",
        "        r'<span style=\"font-size:1.2em; font-weight:bold;\">\\1</span>',\n",
        "        response\n",
        "    )\n",
        "\n",
        "    # Indent lines starting with \"-\" by wrapping them in a div with padding\n",
        "    formatted_response = re.sub(\n",
        "        r\"^- (.*)\",\n",
        "        r'<div style=\"margin-left: 20px;\">- \\1</div>',\n",
        "        formatted_response,\n",
        "        flags=re.MULTILINE\n",
        "    )\n",
        "\n",
        "    # Indent lines starting with numbers (e.g., 1., 2., etc.)\n",
        "    formatted_response = re.sub(\n",
        "        r\"^\\d+\\.\\s(.*)\",\n",
        "        r'<div style=\"margin-left: 20px;\">\\g<0></div>',\n",
        "        formatted_response,\n",
        "        flags=re.MULTILINE\n",
        "    )\n",
        "\n",
        "    formatted_response = formatted_response.replace(\"\\n\", \"<br>\")  # Replace newlines with <br> for HTML\n",
        "\n",
        "    # Ensure no trailing ** or incomplete text\n",
        "    if \"**\" in formatted_response:\n",
        "        formatted_response = formatted_response.replace(\"**\", \"\")  # Remove unclosed asterisks\n",
        "\n",
        "    return formatted_response\n",
        "\n",
        "# Set up the Gradio interface with a user-friendly input label and placeholder\n",
        "interface = gr.Interface(\n",
        "    fn=chatbot,\n",
        "    inputs=gr.Textbox(\n",
        "        label=\"Ask a question about Italian cuisine or share an ingredient!\",  # User-friendly label\n",
        "        placeholder=\"e.g., How do I make pizza milkshake?\",  # Example input for clarity\n",
        "    ),\n",
        "    outputs=\"html\",  # Use \"html\" output for better formatting\n",
        "    title=\"Italian Cuisine Chatbot\",\n",
        "    description=\"Ask me anything about Italian cuisine or cooking!\",\n",
        "    allow_flagging=\"never\"  # Disable the flag button\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "interface.launch(share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BaLeLt3eSFI8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
