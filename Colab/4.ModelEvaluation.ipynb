{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "wSUMWsJEZxg9"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f14ba4a8f07a4f6a8302eaa21195d96f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a3a833c6aff40819d562140bd2f84aa",
              "IPY_MODEL_20021a80c2e8426c9b8ab7b992ded4a7",
              "IPY_MODEL_b7da3afffc9843edbeff2f1bd1759ec6"
            ],
            "layout": "IPY_MODEL_56b2a2486f674985b71128ad8b20ff81"
          }
        },
        "4a3a833c6aff40819d562140bd2f84aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d5d7b66b03f495f92bc14ff97c7eec6",
            "placeholder": "​",
            "style": "IPY_MODEL_09ad0dc2179945f2bd3e81b0b40dfaf7",
            "value": "adapter_model.safetensors: 100%"
          }
        },
        "20021a80c2e8426c9b8ab7b992ded4a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a23f041c0b44a58a8ab623739d6e0ef",
            "max": 45118424,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5ae4d1f455114d51b17033d55cf75688",
            "value": 45118424
          }
        },
        "b7da3afffc9843edbeff2f1bd1759ec6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7772f8f8cbc44003abe79e3c45a8398c",
            "placeholder": "​",
            "style": "IPY_MODEL_1d8a0dc8621441a28801c68055fe6fd1",
            "value": " 45.1M/45.1M [00:01&lt;00:00, 39.1MB/s]"
          }
        },
        "56b2a2486f674985b71128ad8b20ff81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d5d7b66b03f495f92bc14ff97c7eec6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09ad0dc2179945f2bd3e81b0b40dfaf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a23f041c0b44a58a8ab623739d6e0ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ae4d1f455114d51b17033d55cf75688": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7772f8f8cbc44003abe79e3c45a8398c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d8a0dc8621441a28801c68055fe6fd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "model_name = \"davnas/Italian_Cousine_1.3\"\n",
        "dataset_name = \"/content/test_set_new.jsonl\"\n"
      ],
      "metadata": {
        "id": "5tTyYUVnopL5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "#!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
        "\n",
        "!pip install -q bitsandbytes\n",
        "!pip install -q evaluate jsonlines rouge_score bert-score\n",
        "!pip install transformers peft accelerate bitsandbytes jsonlines\n",
        "!pip install evaluate tqdm\n",
        "import evaluate\n",
        "import evaluate\n",
        "import jsonlines\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from tqdm import tqdm  # Import tqdm for progress tracking\n"
      ],
      "metadata": {
        "id": "tsrT3b062A52"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: please download the test set from \"https://github.com/davidenascivera/LAB2_Scalable/blob/main/Test_set/test_set.jsonl\"\n",
        "\n",
        "import requests\n",
        "from io import StringIO\n",
        "import json\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/davidenascivera/LAB2_Scalable/main/Test_set/test_set.jsonl\"\n",
        "\n",
        "try:\n",
        "    response = requests.get(url, stream=True)\n",
        "    response.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "    with open(\"test_set.jsonl\", \"wb\") as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "\n",
        "    print(\"Download successful. File saved as test_set.jsonl\")\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error downloading the file: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtFCmD4TxEUa",
        "outputId": "a9c6391d-bdf0-49c2-ee45-97c9608b0548"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download successful. File saved as test_set.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: please download the test set from \"https://github.com/davidenascivera/LAB2_Scalable/blob/main/Test_set/test_set.jsonl\"\n",
        "\n",
        "import requests\n",
        "from io import StringIO\n",
        "import json\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/davidenascivera/LAB2_Scalable/main/Test_set/test_set_new.jsonl\"\n",
        "\n",
        "try:\n",
        "    response = requests.get(url, stream=True)\n",
        "    response.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "    with open(\"test_set_new.jsonl\", \"wb\") as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "\n",
        "    print(\"Download successful. File saved as test_set.jsonl\")\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error downloading the file: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KVzsqsywvja",
        "outputId": "3c1fe32a-eff8-4d3e-8d96-6afaae011221"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download successful. File saved as test_set.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.SimpleInference"
      ],
      "metadata": {
        "id": "wSUMWsJEZxg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "max_seq_length = 2048  # Choose any! We auto support ROPE scaling internally!\n",
        "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, bFloat16 for Ampere+\n",
        "\n",
        "model_name_or_path = \"davnas/Italian_Cousine\"\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name_or_path,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=True,\n",
        "    # token = \"hf_...\", #se il nostro modello non è public\n",
        "    # Use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "eUKCHB0n2A_K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "b9be3d25-c47d-4c65-8ba0-a2ad66db32f2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom transformers import AutoModel, AutoTokenizer\\n\\nmax_seq_length = 2048  # Choose any! We auto support ROPE scaling internally!\\ndtype = None  # None for auto detection. Float16 for Tesla T4, V100, bFloat16 for Ampere+\\n\\nmodel_name_or_path = \"davnas/Italian_Cousine\"\\n\\nfrom unsloth import FastLanguageModel\\nimport torch\\n\\nmodel, tokenizer = FastLanguageModel.from_pretrained(\\n    model_name=model_name_or_path,\\n    max_seq_length=max_seq_length,\\n    dtype=dtype,\\n    load_in_4bit=True,\\n    # token = \"hf_...\", #se il nostro modello non è public\\n    # Use one if using gated models like meta-llama/Llama-2-7b-hf\\n)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"How can I cook an smoothie?\"},\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,  # Must add for generation\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "model.generate(input_ids=inputs, streamer=text_streamer, max_new_tokens=128,\n",
        "               use_cache=True, temperature=1.5, min_p=0.1)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "G1FIySw52BCi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "d7bc3833-e93a-4e3c-f084-961faea4682a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom unsloth import FastLanguageModel\\n\\nFastLanguageModel.for_inference(model)  # Enable native 2x faster inference\\n\\nmessages = [\\n    {\"role\": \"user\", \"content\": \"How can I cook an smoothie?\"},\\n]\\n\\ninputs = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=True,\\n    add_generation_prompt=True,  # Must add for generation\\n    return_tensors=\"pt\",\\n).to(\"cuda\")\\n\\nfrom transformers import TextStreamer\\ntext_streamer = TextStreamer(tokenizer, skip_prompt=True)\\n\\nmodel.generate(input_ids=inputs, streamer=text_streamer, max_new_tokens=128,\\n               use_cache=True, temperature=1.5, min_p=0.1)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Evaluation"
      ],
      "metadata": {
        "id": "qZmyZX9TZ2MV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_test_data(file_path):\n",
        "    \"\"\"Load test data from JSONL file.\"\"\"\n",
        "    prompts = []\n",
        "    completions = []\n",
        "    with jsonlines.open(file_path) as reader:\n",
        "        for item in reader:\n",
        "            prompts.append(item['prompt'])\n",
        "            completions.append(item['completion'])\n",
        "    return prompts, completions\n",
        "\n"
      ],
      "metadata": {
        "id": "xeeSk2a4aBW0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: clean the memory vram\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "# Clear GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Run garbage collection\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InxFL11meraj",
        "outputId": "e183dcf1-5339-4059-db4f-f717b896be39"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#------------------------------------------\n",
        "\n",
        "def generate_predictions(model, tokenizer, prompts):\n",
        "    \"\"\"Generate model predictions for given prompts with progress tracking.\"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    # Use tqdm to wrap the prompts list for progress tracking\n",
        "    for i, prompt in enumerate(tqdm(prompts, desc=\"Generating Predictions\")):\n",
        "        print(f\"Processing prompt {i + 1}/{len(prompts)}: {prompt}\")\n",
        "\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        # Ensure input_ids are Long type and on cuda\n",
        "        inputs = inputs.to(\"cuda\").to(torch.long)  # Changed this line\n",
        "\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs,\n",
        "            max_new_tokens=512,\n",
        "            use_cache=True,\n",
        "            temperature=1.5,\n",
        "            min_p=0.1\n",
        "        )\n",
        "\n",
        "        pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        predictions.append(pred)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Initialize model\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,  # Use the actual model name\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True\n",
        ")\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Load test data\n",
        "prompts, references = load_test_data(dataset_name)\n",
        "\n",
        "# Generate predictions with progress tracking\n",
        "predictions = generate_predictions(model, tokenizer, prompts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885,
          "referenced_widgets": [
            "f14ba4a8f07a4f6a8302eaa21195d96f",
            "4a3a833c6aff40819d562140bd2f84aa",
            "20021a80c2e8426c9b8ab7b992ded4a7",
            "b7da3afffc9843edbeff2f1bd1759ec6",
            "56b2a2486f674985b71128ad8b20ff81",
            "2d5d7b66b03f495f92bc14ff97c7eec6",
            "09ad0dc2179945f2bd3e81b0b40dfaf7",
            "1a23f041c0b44a58a8ab623739d6e0ef",
            "5ae4d1f455114d51b17033d55cf75688",
            "7772f8f8cbc44003abe79e3c45a8398c",
            "1d8a0dc8621441a28801c68055fe6fd1"
          ]
        },
        "id": "mpBobqbKjzOR",
        "outputId": "9fb77192-5baa-495f-ed4d-63d94e7d370c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2024.12.3: Fast Llama patching. Transformers:4.46.3.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/45.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f14ba4a8f07a4f6a8302eaa21195d96f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:   0%|          | 0/40 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 1/40: How can I make a hearty vegetable stir-fry?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:   2%|▎         | 1/40 [00:13<08:42, 13.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 2/40: What’s the recipe for creamy mushroom soup?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:   5%|▌         | 2/40 [00:19<05:52,  9.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 3/40: How do I prepare a classic Caesar salad?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:   8%|▊         | 3/40 [00:24<04:33,  7.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 4/40: What’s the method to cook lemon garlic salmon?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  10%|█         | 4/40 [00:32<04:24,  7.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 5/40: How can I make a delicious chicken Alfredo pasta?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  12%|█▎        | 5/40 [00:39<04:14,  7.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 6/40: What’s the best way to prepare beef tacos?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  15%|█▌        | 6/40 [00:44<03:46,  6.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 7/40: How do I make a fluffy omelette?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  18%|█▊        | 7/40 [00:50<03:33,  6.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 8/40: What’s the recipe for homemade guacamole?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  20%|██        | 8/40 [01:02<04:16,  8.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 9/40: How can I bake a chocolate chip cookie?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  22%|██▎       | 9/40 [01:09<04:00,  7.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 10/40: What’s the best way to make beef stew?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  25%|██▌       | 10/40 [01:15<03:35,  7.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 11/40: How do I prepare a spinach and feta quiche?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  28%|██▊       | 11/40 [01:24<03:44,  7.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 12/40: What’s the method to cook shrimp scampi?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  30%|███       | 12/40 [01:31<03:29,  7.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 13/40: How can I bake banana bread?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  32%|███▎      | 13/40 [01:36<03:04,  6.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 14/40: What’s the recipe for homemade pizza Margherita?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  35%|███▌      | 14/40 [01:39<02:28,  5.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 15/40: How do I make a refreshing fruit smoothie?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  38%|███▊      | 15/40 [01:45<02:27,  5.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 16/40: What’s the best way to cook chicken tikka masala?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  40%|████      | 16/40 [02:00<03:21,  8.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 17/40: How can I make a creamy pumpkin soup?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  42%|████▎     | 17/40 [02:05<02:51,  7.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 18/40: What’s the recipe for garlic butter shrimp?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  45%|████▌     | 18/40 [02:12<02:41,  7.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 19/40: How can I prepare a vegetarian lasagna?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  48%|████▊     | 19/40 [02:20<02:35,  7.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 20/40: What’s the best way to make blueberry muffins?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  50%|█████     | 20/40 [02:28<02:33,  7.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 21/40: How do I make a creamy chicken Alfredo sauce?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  52%|█████▎    | 21/40 [02:34<02:15,  7.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 22/40: What’s the recipe for baked salmon with herbs?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  55%|█████▌    | 22/40 [02:41<02:06,  7.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 23/40: How can I make a vegetarian chili?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  57%|█████▊    | 23/40 [02:46<01:50,  6.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 24/40: What’s the method to make a vanilla panna cotta?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  60%|██████    | 24/40 [02:53<01:48,  6.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 25/40: How do I make a Greek salad?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  62%|██████▎   | 25/40 [02:59<01:38,  6.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 26/40: What’s the best way to prepare a tofu stir-fry?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  65%|██████▌   | 26/40 [03:07<01:35,  6.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 27/40: How can I make a blueberry lemon tart?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  68%|██████▊   | 27/40 [03:13<01:24,  6.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 28/40: What’s the best way to make ratatouille?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  70%|███████   | 28/40 [03:19<01:18,  6.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 29/40: How do I make a chocolate mousse?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  72%|███████▎  | 29/40 [03:27<01:17,  7.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 30/40: What’s the recipe for beef and broccoli stir-fry?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  75%|███████▌  | 30/40 [03:34<01:09,  6.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 31/40: How can I make a creamy potato gratin?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  78%|███████▊  | 31/40 [03:39<00:57,  6.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 32/40: What’s the best way to make a caprese salad?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  80%|████████  | 32/40 [03:46<00:51,  6.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 33/40: How do I make a vegan lentil curry?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  82%|████████▎ | 33/40 [03:51<00:43,  6.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 34/40: What’s the recipe for banana pancakes?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  85%|████████▌ | 34/40 [03:54<00:30,  5.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 35/40: How do I make a refreshing cucumber salad?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  88%|████████▊ | 35/40 [04:01<00:27,  5.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 36/40: What’s the best way to cook stuffed bell peppers?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  90%|█████████ | 36/40 [04:07<00:22,  5.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 37/40: How can I make a classic French omelette?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  92%|█████████▎| 37/40 [04:14<00:18,  6.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 38/40: What’s the recipe for creamy garlic mashed potatoes?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  95%|█████████▌| 38/40 [04:20<00:12,  6.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 39/40: How do I make a simple bruschetta appetizer?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating Predictions:  98%|█████████▊| 39/40 [04:26<00:06,  6.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 40/40: What’s the best way to make a shrimp and avocado salad?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Predictions: 100%|██████████| 40/40 [04:31<00:00,  6.79s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate using ROUGE\n",
        "rouge = evaluate.load('rouge')\n",
        "rouge_results = rouge.compute(\n",
        "    predictions=predictions,\n",
        "    references=references,\n",
        "    use_stemmer=True\n",
        ")\n",
        "\n",
        "# Evaluate using BERTScore\n",
        "bertscore = evaluate.load('bertscore')\n",
        "bert_results = bertscore.compute(\n",
        "    predictions=predictions,\n",
        "    references=references,\n",
        "    lang=\"en\",\n",
        "    model_type=\"microsoft/deberta-xlarge-mnli\"\n",
        ")\n",
        "\n",
        "print(f\"Dataset: {dataset_name}\")\n",
        "print(f\"Model: {model_name}\")\n",
        "\n",
        "# Print results\n",
        "print(\"\\nROUGE Scores:\")\n",
        "for metric, score in rouge_results.items():\n",
        "    print(f\"{metric}: {score:.4f}\")\n",
        "\n",
        "print(\"\\nBERTScore:\")\n",
        "print(f\"Precision: {sum(bert_results['precision']) / len(bert_results['precision']):.4f}\")\n",
        "print(f\"Recall: {sum(bert_results['recall']) / len(bert_results['recall']):.4f}\")\n",
        "print(f\"F1: {sum(bert_results['f1']) / len(bert_results['f1']):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVnoZEQNaNWI",
        "outputId": "d353251a-d655-4568-bed3-9bf919e07719"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: /content/test_set_new.jsonl\n",
            "Model: davnas/Italian_Cousine_1.3\n",
            "\n",
            "ROUGE Scores:\n",
            "rouge1: 0.4347\n",
            "rouge2: 0.1312\n",
            "rougeL: 0.2832\n",
            "rougeLsum: 0.4289\n",
            "\n",
            "BERTScore:\n",
            "Precision: 0.6936\n",
            "Recall: 0.6919\n",
            "F1: 0.6920\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C11zJwcB0WNZ"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}
